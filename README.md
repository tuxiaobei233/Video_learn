# 对比学习

在原始数据集中，我们有很多样本，编号为 0，1，2（非谣言，谣言，辟谣）。为了提高我们在向量数据库中检索的准确性，我们需要能很好地通过向量区分出这些样本。我们随机将样本分为两两一对，作为一个数据训练，其标签为0=两个样本标签不同，1=两个样本标签不同。在这种情况下，我们可以使用对比性损失： 标签为1的相似对被拉到一起，，因此它们在向量空间中是接近的。不相似的对，如果比定义的边际更接近，则在向量空间中被推开。

也就是我们实现了相同标签的样本在向量空间中尽可能聚集，同时与不同标签的样本相隔的距离尽可能远。

我们使用cosine_distance（也就是1-cosine_similarity）作为我们基础性的对比损失函数，边距为0.5。也就是说，不相同标签的样本应该有至少0.5的余弦距离（相当于0.5的余弦相似度差异）。

对比性损失的改进版本是OnlineContrastiveLoss，它寻找哪些消极对的距离低于最大的积极对，哪些积极对的距离高于消极对的最低距离。也就是说，这种损失会自动检测一批中的困难案例，并只对这些案例计算损失。
